{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SegFormer_modified.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOLKmip1Wr2hIdlqLyia6cS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"po4YpJc2S_6U"},"outputs":[],"source":["import os\n","import tempfile\n","import numpy as np\n","import tensorflow as tf\n","from PIL import Image"]},{"cell_type":"code","source":["!pip install wget"],"metadata":{"id":"dtDQqww9THtl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/sithu31296/semantic-segmentation.git"],"metadata":{"id":"m9Gg1HWwvegD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gdown\n","from pathlib import Path\n","\n","ckpt = Path('./checkpoints/pretrained/segformer')\n","ckpt.mkdir(exist_ok=True, parents=True)\n","\n","url = 'https://drive.google.com/uc?id=1-OmW3xRD3WAbJTzktPC-VMOF5WMsN8XT'\n","output = './checkpoints/pretrained/segformer/segformer.b3.ade.pth'\n","\n","gdown.download(url, output, quiet=False)"],"metadata":{"id":"mv66qS27TMrF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from semseg.models import *\n","\n","model = eval('SegFormer')(\n","    backbone='MiT-B5',\n","    num_classes=150\n",")"],"metadata":{"id":"Zu7qlkjovYXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\n","    graph_def_file = model, \n","    input_arrays = ['sub_2'],\n","    output_arrays = ['ResizeBilinear_2']\n",")\n","\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","\n","tflite_model = converter.convert()\n","\n","_, tflite_path = tempfile.mkstemp('.tflite')\n","tflite_model_size = open(tflite_path, 'wb').write(tflite_model)\n","tf_model_size = os.path.getsize(model)"],"metadata":{"id":"2xjVQsplTNwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["interpreter = tf.lite.Interpreter(model_path=url)\n","\n","input_details = interpreter.get_input_details()\n","interpreter.allocate_tensors()\n","\n","input_size = input_details[0]['shape'][2], input_details[0]['shape'][1]\n","print(input_size)"],"metadata":{"id":"NW2q1HEfTOMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","image = Image.open('/content/drive/MyDrive/Datasets/stuttgart_000131_000019_leftImg8bit.png')\n","image"],"metadata":{"id":"Flhlr2TETOT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import ImageOps\n","\n","old_size = image.size \n","desired_ratio = input_size[0] / input_size[1]\n","old_ratio = old_size[0] / old_size[1]\n","\n","if old_ratio < desired_ratio:\n","    new_size = (old_size[0], int(old_size[0] / desired_ratio))\n","else:\n","    new_size = (int(old_size[1] * desired_ratio), old_size[1])\n","\n","print(new_size, old_size)\n","\n","delta_w = new_size[0] - old_size[0]\n","delta_h = new_size[1] - old_size[1]\n","padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n","cropped_image = ImageOps.expand(image, padding)\n","cropped_image"],"metadata":{"id":"Ctcm2Ga_TOrS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resized_image = cropped_image.convert('RGB').resize(input_size, Image.BILINEAR)\n","\n","image_for_prediction = np.asarray(resized_image).astype(np.float32)\n","image_for_prediction = np.expand_dims(image_for_prediction, 0)\n","image_for_prediction = image_for_prediction / 127.5 - 1"],"metadata":{"id":"bl-nbSwETOzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["interpreter = tf.lite.Interpreter(model_path=url)\n","\n","interpreter.allocate_tensors()\n","interpreter.set_tensor(input_details[0]['index'], image_for_prediction)\n","interpreter.invoke()\n","\n","raw_prediction = interpreter.tensor(\n","    interpreter.get_output_details()[0]['index'])()\n","\n","width, height = cropped_image.size\n","seg_map = tf.argmax(tf.image.resize(raw_prediction, (height, width)), axis=3)\n","seg_map = tf.squeeze(seg_map).numpy().astype(np.int8)"],"metadata":{"id":"TqfJ7VUHTO71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib import gridspec\n","from matplotlib import pyplot as plt\n","\n","def create_cityscapes_label_colormap():\n","  colormap = np.zeros((256, 3), dtype=np.uint8)\n","  colormap[0] = [128, 64, 128]\n","  colormap[1] = [244, 35, 232]\n","  colormap[2] = [70, 70, 70]\n","  colormap[3] = [102, 102, 156]\n","  colormap[4] = [190, 153, 153]\n","  colormap[5] = [153, 153, 153]\n","  colormap[6] = [250, 170, 30]\n","  colormap[7] = [220, 220, 0]\n","  colormap[8] = [107, 142, 35]\n","  colormap[9] = [152, 251, 152]\n","  colormap[10] = [70, 130, 180]\n","  colormap[11] = [220, 20, 60]\n","  colormap[12] = [255, 0, 0]\n","  colormap[13] = [0, 0, 142]\n","  colormap[14] = [0, 0, 70]\n","  colormap[15] = [0, 60, 100]\n","  colormap[16] = [0, 80, 100]\n","  colormap[17] = [0, 0, 230]\n","  colormap[18] = [119, 11, 32]\n","  return colormap\n","\n","\n","def label_to_color_image(label):\n","  if label.ndim != 2:\n","    raise ValueError('Expect 2-D input label')\n","\n","  colormap = create_cityscapes_label_colormap()\n","\n","  if np.max(label) >= len(colormap):\n","    raise ValueError('label value too large.')\n","\n","  return colormap[label]\n","\n","'''\n","def vis_segmentation(image, seg_map):\n","  plt.figure(figsize=(15, 5))\n","  grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n","\n","  plt.subplot(grid_spec[0])\n","  plt.imshow(image)\n","  plt.axis('off')\n","  plt.title('input image')\n","\n","  plt.subplot(grid_spec[1])\n","  seg_image = label_to_color_image(seg_map).astype(np.uint8)\n","  plt.imshow(seg_image)\n","  plt.axis('off')\n","  plt.title('segmentation map')\n","\n","  plt.subplot(grid_spec[2])\n","  plt.imshow(image)\n","  plt.imshow(seg_image, alpha=0.7)\n","  plt.axis('off')\n","  plt.title('segmentation overlay')\n","\n","  unique_labels = np.unique(seg_map)\n","  ax = plt.subplot(grid_spec[3])\n","  plt.imshow(\n","      FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n","  ax.yaxis.tick_right()\n","  plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n","  plt.xticks([], [])\n","  ax.tick_params(width=0.0)\n","  plt.grid('off')\n","  plt.show()\n","'''\n","\n","LABEL_NAMES = np.asarray([\n","      'road',\n","      'sidewalk',\n","      'building',\n","      'wall',\n","      'fence',\n","      'pole',\n","      'traffic light',\n","      'traffic sign',\n","      'vegetation',\n","      'terrain',\n","      'sky',\n","      'person',\n","      'rider',\n","      'car',\n","      'truck',\n","      'bus',\n","      'train',\n","      'motorcycle',\n","      'bicycle',\n","])\n","\n","FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n","FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"],"metadata":{"id":"1M5EZrO-TPC-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image=cropped_image\n","plt.figure(figsize=(15, 5))\n","grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n","\n","plt.subplot(grid_spec[0])\n","plt.imshow(image)\n","plt.axis('off')\n","plt.title('input image')\n","\n","plt.subplot(grid_spec[1])\n","seg_image = label_to_color_image(seg_map).astype(np.uint8)\n","plt.imshow(seg_image)\n","plt.axis('off')\n","plt.title('segmentation map')\n","\n","plt.subplot(grid_spec[2])\n","plt.imshow(image)\n","plt.imshow(seg_image, alpha=0.6)\n","plt.axis('off')\n","plt.title('segmentation overlay')\n","\n","unique_labels = np.unique(seg_map)\n","ax = plt.subplot(grid_spec[3])\n","plt.imshow(\n","    FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n","ax.yaxis.tick_right()\n","plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n","plt.xticks([], [])\n","ax.tick_params(width=0.0)\n","plt.grid('off')\n","plt.show()"],"metadata":{"id":"IXCHDRayTPKD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["segmented_image=Image.fromarray(seg_image, 'RGB')\n","segmented_image"],"metadata":{"id":"IGhNOwwYTPQr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["segmented_image.save(\"/content/drive/MyDrive/Datasets/pred.jpg\")"],"metadata":{"id":"tN6c_zqtTPWT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gt=Image.open('/content/drive/MyDrive/Datasets/stuttgart_000131_000019_gtFine_color.png')\n","gt"],"metadata":{"id":"EooMjIaZTPbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","from sklearn import metrics"],"metadata":{"id":"RkySD5deTPgy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["truth=cv2.imread(\"/content/drive/MyDrive/Datasets/stuttgart_000131_000019_gtFine_color.png\", 0).reshape(-1)\n","pred=cv2.imread(\"/content/drive/MyDrive/Datasets/pred.jpg\", 0).reshape(-1)"],"metadata":{"id":"8mcNjfr4TPmL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["truth.shape"],"metadata":{"id":"4BjSlYQSTPrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred.shape"],"metadata":{"id":"RqY_qFT-TPwL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["truth = np.resize(truth, (2096128,))"],"metadata":{"id":"H_yq9mNiTP1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A=truth\n","B=pred\n","print(A.shape)\n","print(B.shape)"],"metadata":{"id":"Qi1vBjdXTP58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len1=len(truth)\n","len2=len(pred)\n","if len1<len2:\n","  truth=np.append(truth, np.zeros((len2-len1)),axis=0)\n","elif len2<len1:\n","  pred=np.append(pred, np.zeros((len1-len2)),axis=0)\n","print(truth.shape)\n","print(pred.shape)\n","if (truth.shape != pred.shape):\n","  print(\"we have an error\")\n","else:\n","  print(\"we r good\")"],"metadata":{"id":"ZkDR-77cTP-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if A.shape[0] < B.shape[0]:\n","    A = np.vstack((A, np.zeros((B.shape[0] - A.shape[0]))))\n","elif A.shape[0] > B.shape[0]:\n","    B = np.vstack((B, np.zeros((A.shape[0] - B.shape[0], A.shape[0])))) \n","\n","print(A.shape)\n","print(B.shape)"],"metadata":{"id":"_1fUfu3NTQDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.metrics import MeanIoU"],"metadata":{"id":"nxbD4YZqTQH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred1=pred/226\n","truth1=truth/194"],"metadata":{"id":"UQoIyYH3TQMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mxresult=np.amax(pred1)\n","mxresult"],"metadata":{"id":"cXMYAE7cleM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mxresult=np.amax(truth1)\n","mxresult"],"metadata":{"id":"MUaeLL0mleSm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes=19\n","IOU_keras=MeanIoU(num_classes=num_classes)\n","IOU_keras.update_state(truth1, pred1)\n","print(IOU_keras.result().numpy())"],"metadata":{"id":"xHbVZ3jYleY4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%matplotlib inline\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"neZV_xhIleeu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.heatmap(IOU_keras, annot = True)\n","plt.xlabel('Predicted label')\n","plt.ylabel('True label')"],"metadata":{"id":"8Y4fFZG-lej4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"oxtiaR2Slepq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0wxCyqQjleuy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"8371yGL0le1o"},"execution_count":null,"outputs":[]}]}